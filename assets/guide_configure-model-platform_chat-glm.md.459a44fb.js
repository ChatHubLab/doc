import{_ as t,o as a,c as e,Q as o}from"./chunks/framework.4719a631.js";const b=JSON.parse('{"title":"koishi-plugin-chathub-chatglm-adapter","description":"","frontmatter":{},"headers":[],"relativePath":"guide/configure-model-platform/chat-glm.md","filePath":"guide/configure-model-platform/chat-glm.md","lastUpdated":1689603033000}'),d={name:"guide/configure-model-platform/chat-glm.md"},h=o('<h1 id="koishi-plugin-chathub-chatglm-adapter" tabindex="-1">koishi-plugin-chathub-chatglm-adapter <a class="header-anchor" href="#koishi-plugin-chathub-chatglm-adapter" aria-label="Permalink to &quot;koishi-plugin-chathub-chatglm-adapter&quot;">​</a></h1><p><a href="https://www.npmjs.com/package/@dingyi222666/koishi-plugin-chathub-chatglm-adapter" target="_blank" rel="noreferrer"><img src="https://img.shields.io/npm/v/@dingyi222666/koishi-plugin-chathub-chatglm-adapter" alt="npm"></a></p><blockquote><p>这是一个为 <a href="https://github.com/ChatHubLab/chathub" target="_blank" rel="noreferrer">ChatHub</a> 提供 <a href="https://github.com/THUDM/ChatGLM-6B" target="_blank" rel="noreferrer">ChatGLM</a> 支持的适配器插件</p></blockquote><p>本章节将介绍如何使用本插件来实现与 ChatGLM 的聊天互动。本章节包括以下内容：</p><ul><li><a href="#如何使用">如何使用？</a></li><li><a href="#常见问题">常见问题</a></li><li><a href="#配置项">配置项</a></li></ul><h2 id="如何使用" tabindex="-1">如何使用？ <a class="header-anchor" href="#如何使用" aria-label="Permalink to &quot;如何使用？&quot;">​</a></h2><ol><li>你需要<strong>自己搭建 ChatGLM 后端服务</strong>，这个插件基于<a href="https://github.com/xusenlinzy/api-for-open-llm" target="_blank" rel="noreferrer">这个</a>后端服务，所以请按照这个项目的说明来搭建后端服务（如果有其他人已经搭建了这个后端服务，你也可以使用他们的后端服务）。</li><li>在插件市场安装这个插件(<code>chathub-chatglm-adapter</code>)，并且安装好这个插件依赖的前置插件。</li><li>在插件配置中填写你的后端服务（或者他人搭建的公开的）的访问地址，以及你的后端服务的访问 token。</li></ol><div class="warning custom-block"><p class="custom-block-title">注意</p><p><strong>由于这个插件的网络请求依然是基于<code>core</code>插件的代理设置配置的，如果你是本地搭建的后端服务，注意不要让代理给代理了你的后端服务的请求地址（<code>localhost</code>，局域网）</strong></p></div><h2 id="常见问题" tabindex="-1">常见问题 <a class="header-anchor" href="#常见问题" aria-label="Permalink to &quot;常见问题&quot;">​</a></h2><h3 id="搭建后端服务的最低配置要求是什么" tabindex="-1">搭建后端服务的最低配置要求是什么？ <a class="header-anchor" href="#搭建后端服务的最低配置要求是什么" aria-label="Permalink to &quot;搭建后端服务的最低配置要求是什么？&quot;">​</a></h3><p>最低要求是拥有 6GB 显存的消费级显卡（N卡优先），推荐使用 12G 显存的显卡。</p><p>如果你的本地配置不够，你可以自行尝试在 AutoDL 等平台上搭建后端服务。</p><p>对话的历史记录是保存在本地客户端上的，因此只需要运行起后端服务就可以了。</p><h2 id="配置项" tabindex="-1">配置项 <a class="header-anchor" href="#配置项" aria-label="Permalink to &quot;配置项&quot;">​</a></h2><p>这是一个介绍 <code>koishi-plugin-chathub-chatglm-adapter</code> 插件的配置项的文档。这个插件的配置项可以分为三类：请求设置、模型设置和前置插件设置。下面我们分别介绍每一类配置项的含义和用法。</p><h3 id="全局设置" tabindex="-1">全局设置 <a class="header-anchor" href="#全局设置" aria-label="Permalink to &quot;全局设置&quot;">​</a></h3><table><thead><tr><th>配置项</th><th>类型</th><th>必填</th><th>默认值</th><th>作用</th></tr></thead><tbody><tr><td>chatConcurrentMaxSize</td><td><code>number</code></td><td>否</td><td><code>1</code></td><td>设置当前适配器适配的模型的最大并发聊天数</td></tr><tr><td>chatTimeLimit</td><td><code>number</code> 或 <code>Computed&lt;Awaitable&lt;number&gt;&gt;</code></td><td>否</td><td><code>20</code></td><td>设置每小时的调用限额（次数）</td></tr><tr><td>timeout</td><td><code>number</code></td><td>否</td><td><code>200000</code></td><td>设置请求超时时间（毫秒）</td></tr><tr><td>maxRetries</td><td><code>number</code></td><td>否</td><td><code>3</code></td><td>设置模型请求失败后的最大重试次数</td></tr></tbody></table><h3 id="请求设置" tabindex="-1">请求设置 <a class="header-anchor" href="#请求设置" aria-label="Permalink to &quot;请求设置&quot;">​</a></h3><p>请求设置是指与 ChatGLM 后端服务进行通信时所需要的参数。这些参数包括：</p><ul><li><code>apiEndPoint</code>: 这是一个字符串，表示请求 ChatGLM 自搭建后端的 API 地址。这个地址必须是有效的 URL，且不能包含查询参数或哈希值。例如：<code>https://example.com/api/chatglm</code>。这个参数是必填的。</li><li><code>apiKey</code>: 这是一个字符串，表示 ChatGLM 自搭建后端的身份验证 API key。这个 key 是用于验证请求者身份的密钥，一般由后端服务提供者提供。这个参数也是必填的，且应该保密，不要泄露给他人。</li></ul><h3 id="模型设置" tabindex="-1">模型设置 <a class="header-anchor" href="#模型设置" aria-label="Permalink to &quot;模型设置&quot;">​</a></h3><p>模型设置是指与 ChatGLM 模型本身相关的参数。这些参数可以影响模型生成回复的效果和风格。这些参数包括：</p><ul><li><code>maxTokens</code>: 这是一个数字，表示回复的最大 token 数（16~512，必须是 16 的倍数）。token 是模型内部用来表示文本的基本单位，一般来说，token 数越多，回复的长度越长。这个参数默认值为 256。</li><li><code>temperature</code>: 这是一个数字，表示回复温度，越高越随机（0~1）。温度是模型生成回复时使用的一个概率分布参数，一般来说，温度越高，回复越多样化和创新，但也可能出现不合理或不连贯的内容；温度越低，回复越保守和一致，但也可能出现重复或无聊的内容。这个参数默认值为 0.8。</li></ul><hr><p>总结：</p><p>本章节介绍了如何使用 <code>koishi-plugin-chathub-chatglm-adapter</code> 插件来实现与 ChatGLM 的聊天互动。本章节包括了以下内容：</p><ul><li>如何安装和配置本插件</li><li>如何解决一些常见问题</li><li>如何定制本插件的配置项</li></ul><p>希望本章节对你有所帮助，如果你有任何问题或建议，请随时联系我们。谢谢！</p>',28),l=[h];function r(i,c,n,p,u,s){return a(),e("div",null,l)}const g=t(d,[["render",r]]);export{b as __pageData,g as default};
